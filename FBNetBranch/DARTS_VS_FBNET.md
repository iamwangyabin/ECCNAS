### Experiment settings:
* Take a simple network, which performs well on a particular task. On CIFAR-10 in our case
* Find a convolution such that itâ€™s insertion instead of one convolution with the same number of parameters would highly downgrade all network performance
* Apply DARTS and FBNet (optimization step, without the use of latency) to choose one of the 2 layers (original GOOD VS BAD layer)
* I expected that both supernets will confidently select the GOOD one.

### Illustration of the architecture

<p align="center">
<img src="pictures/experiment_architecture.png" alt="experiment_architecture" width=50% />
</p>
<p align="center"> A simple architecture to test NAS performance on a partial selection </p>

* The highlighted blocks consist of almost the same number of parameters (with 2% gap)
* The GOOD CHOICE model gets 86% accuracy (in 200 epochs)
* The BAD CHOICE model ceiling is 76% accuracy
* I expect that a NAS will not hesitate about the choice of the GOOD model

### An experiment with DARTS

<p align="center">
<img src="pictures/experiment_darts.png" alt="experiment_darts" width=35% />
</p>
<p align="center"> Green line - weights of the GOOD model; Grey line - weights of the BAD model </p>

### An experiment with FBNet

<p align="center">
<img src="pictures/experiment_fbnet.png" alt="experiment_fbnet" width=35% />
</p>
<p align="center"> Blue line - weights of the GOOD model; Red line - weights of the BAD model </p>

#### Conclusions from the experiment:
* Even in the easy-separating case, FBNet highly fluctuates and this explains the general recommendation to sample architecture from the searched distribution and train several searched examples
* The DARTS model performed very confident and found the right configuration in the beginning
* You should always plot weights of multilayers
